{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc59db89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from digitModel import Model\n",
    "from datasetClass import MNISTDataset\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43062884",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n",
    "df_train = pd.read_csv('digit-recognizer/train.csv')\n",
    "df_test = pd.read_csv('digit-recognizer/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "231da1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing std and mean for image preprocessing later to improve performance\n",
    "train_data = df_train.drop('label', axis=1).values\n",
    "train_mean = train_data.mean()/255.\n",
    "train_std = train_data.std()/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59ce7763",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.rand(len(df_train)) < 0.8\n",
    "df_val = df_train[~mask]\n",
    "df_train = df_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e4e8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "classes = range(10)\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.RandomRotation(30),\n",
    "                    transforms.RandomAffine(degrees=20, translate=(0.1,0.1), scale=(0.9, 1.1)),\n",
    "                    transforms.GaussianBlur(kernel_size = 3, sigma=(0.1, 2.0)),\n",
    "                    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[train_mean], std=[train_std]),\n",
    "                    ])\n",
    "\n",
    "\n",
    "val_transform = transforms.Compose(\n",
    "                    [\n",
    "                    transforms.ToPILImage(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[train_mean], std=[train_std]),\n",
    "                    ])\n",
    "test_transform = val_transform\n",
    "\n",
    "train_dataset = MNISTDataset(df_train, transform = train_transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                batch_size=batch_size,shuffle = True)\n",
    "val_dataset = MNISTDataset(df_val, transform = val_transform)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                batch_size=batch_size,shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9152e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "model.to(device)\n",
    "criterion = nn.NLLLoss()   # with log_softmax() as the last layer, this is equivalent to cross entropy loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36ae9f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_losses, val_losses = [], []\n",
    "train_accu, val_accu = [], []\n",
    "start_time = time.time()\n",
    "early_stop_counter = 10   # stop when the validation loss does not improve for 10 iterations to prevent overfitting\n",
    "counter = 0\n",
    "best_val_loss = float('Inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d578f111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10..  Time: 151.62s.. Training Loss: 1.663..  Training Accu: 0.426..  Val Loss: 0.414..  Val Accu: 0.907\n",
      "Epoch: 2/10..  Time: 163.52s.. Training Loss: 1.178..  Training Accu: 0.610..  Val Loss: 0.277..  Val Accu: 0.930\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9400/2947409971.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_ps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\kaggle\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    epoch_start_time = time.time()\n",
    "    running_loss = 0\n",
    "    accuracy=0\n",
    "    # training step\n",
    "    model.train()\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        log_ps = model(images)\n",
    "        \n",
    "        ps = torch.exp(log_ps)                \n",
    "        top_p, top_class = ps.topk(1, dim=1)\n",
    "        equals = top_class == labels.view(*top_class.shape)\n",
    "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        \n",
    "        loss = criterion(log_ps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "      \n",
    "    # record training loss and accuracy\n",
    "    train_losses.append(running_loss/len(train_loader))\n",
    "    train_accu.append(accuracy/len(train_loader))\n",
    "    \n",
    "    \n",
    "    #validation step\n",
    "    val_loss = 0\n",
    "    accuracy=0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            log_ps = model(images)\n",
    "            val_loss += criterion(log_ps, labels)\n",
    "\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "            equals = top_class == labels.view(*top_class.shape)\n",
    "            accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "            \n",
    "    # record validation loss and accuracy\n",
    "    val_losses.append(val_loss/len(val_loader))\n",
    "    val_accu.append(accuracy/len(val_loader))\n",
    "    \n",
    "    print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "          \"Time: {:.2f}s..\".format(time.time()-epoch_start_time),\n",
    "          \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "          \"Training Accu: {:.3f}.. \".format(train_accu[-1]),\n",
    "          \"Val Loss: {:.3f}.. \".format(val_losses[-1]),\n",
    "          \"Val Accu: {:.3f}\".format(val_accu[-1])\n",
    "         )\n",
    "\n",
    "    \n",
    "    #Get best run and stop if training does not improve (early_stop_counter == patience)\n",
    "    if val_losses[-1] < best_val_loss:\n",
    "        best_val_loss = val_losses[-1]\n",
    "        counter=0\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    else:\n",
    "        counter+=1\n",
    "        print('Validation loss has not improved since: {:.3f}..'.format(best_val_loss), 'Count: ', str(counter))\n",
    "        if counter >= early_stop_counter:\n",
    "            print('Early Stopping Now!!!!')\n",
    "            model.load_state_dict(best_model_wts)\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = df_test.values\n",
    "x_test = x_test.reshape([-1, 28, 28]).astype(np.float64)\n",
    "x_test = x_test/255.\n",
    "x_test = (x_test-train_mean)/train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405f8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.expand_dims(x_test, axis=1)\n",
    "x_test = torch.from_numpy(x_test).float().to(device)\n",
    "# x_test.shape\n",
    "x_test.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79580b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction time!\n",
    "model.eval()   # this is needed to disable dropouts\n",
    "with torch.no_grad():    # turn off gradient computation because we don't need it for prediction\n",
    "    ps = model(x_test)\n",
    "    prediction = torch.argmax(ps, 1)\n",
    "    print('Prediction',prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
